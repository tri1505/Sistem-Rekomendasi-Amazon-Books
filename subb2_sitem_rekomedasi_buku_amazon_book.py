# -*- coding: utf-8 -*-
"""Subb2- sitem rekomedasi buku - amazon_book.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lZHaSOCxowcLUCmDIS54XfXc04owP380

# **Data Understanding**

#### **Berikut adalah beberapa tahapan untuk memahami data:**


*   Data Loading
*   Univariate Exploratory Data Analysis
*   Data Preprocessing

### **Import libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tabulate import tabulate

"""### **Data Loading**
Pada Fase ini, data akan di loading dari situs kaggel yang menyediakan dataset Data online untuk buku-buku dari Amazon beserta peringkat pengguna dan pengguna yang membelinya, terdapat 3 file dataset di dalam folder yaitu Books, Ratings, dan Users yang akan digunakan untuk proses pengembangan model.
"""

!kaggle datasets download -d saurabhbagchi/books-dataset

!unzip /content/books-dataset.zip

"""### **Load the dataset**
Baca dataset dengan menggunakan fungsi pandas.read_csv. Implementasikan kode berikut.
"""

books_df = pd.read_csv("/content/books_data/books.csv", encoding='latin-1', on_bad_lines='skip', delimiter=';')
books_df.sample(5)

ratings_df = pd.read_csv("/content/books_data/ratings.csv", encoding='latin-1', on_bad_lines='skip', delimiter=';')
ratings_df.sample(5)

users_df = pd.read_csv("/content/books_data/users.csv", encoding='latin-1', on_bad_lines='skip', delimiter=';')
users_df.sample(5)

print('Jumlah data buku:', len(books_df.ISBN.unique()))
print('Jumlah data rating buku dari pembaca:', len(ratings_df.ISBN.unique()))
print('jumlah data pengguna:', len(users_df['User-ID'].unique()))

"""Berdasarkan output diatas, diperoleh informasi sebagai berikut:

Variabel books memiliki 271.360 jenis buku dan terdiri dari 8 kolom yaitu:
1. ISBN : merupakan nomor identitas unik buku.
2. Book-Title : merupakan judul buku.
3. Book-Author : merupakan nama penulis buku.
4. Year-Of-Publication : merupakan tahun publikasi buku.
5. Publisher : merupakan nama penerbit buku.
6. Image-URL-S : merupakan link URL gambar untuk ukuran small (kecil).
7. Image-URL-M : merupakan link URL gambar untuk ukuran medium (sedang).
8. Image-URL-L : merupakan link URL gambar untuk ukuran large (besar).

Variabel ratings memiliki 340.556 penilaian terhadap buku dan terdiri dari 3 kolom yaitu:
1. User-ID : merupakan kode unik untuk nama pengguna anonim yang memberikan penilaian.
2. ISBN : merupakan nomor identitas buku.
3. Book-Rating : merupakan penilaian yang diberikan kepada buku.

Variabel users memiliki 278.858 nama pengguna anonim dan terdiri dari 3 kolom yaitu:
1. User-ID : merupakan kode unik untuk nama pengguna anonim.
2. Location : merupakan lokasi tempat tinggal pengguna.
3. Age : merupakan usia pengguna.

### **Univariate Exploratory Data Analysis**

Pada Fase ini,  dilakukan analisis dan eksplorasi pada setiap variabel untuk memahami distribusi dan karakteristik individu dari variabel tersebut. Pemahaman ini nantinya akan membantu dalam menentukan pendekatan atau algoritma yang cocok diterapkan pada data. Variabel - variabel pada Book Recommendation Dataset adalah sebagai berikut:

1. books : merupakan data yang berisi informasi buku.
2. ratings : merupakan rating atau peringkat yang diberikan ke buku oleh pengguna atau pembaca
3. users : merupakan informasi pengguna termasuk informasi demografisnya.

### **1. Dataset Books**
Pertama, periksa terlebih dahulu isi dari dataset  books dan cek informasi dari dataset books menggunakan fungsi info().
"""

# cek informasi dataset
books_df.info()

"""Berdasarkan output, diketahui bahwa file books.csv memiliki 271.360 entri dan terdiri dari 8 kolom yaitu ISBN, Book-Title, Book-Author, Year-Of-Publication, Publisher, Image-URL-S, Image-URL-M, dan Image-URL-L. Diketahui juga bahwa kolom 'Year-Of-Publication' bertipe data object sedangkan tahun publikasi pada umumnya memiliki tipe data integer. Oleh karena itu akan dilakukan perbaikan tipe data terlebih dahulu.

ketika menjalankan kode berikut:
> **books['Year-Of-Publication'].astype('int')**

Untuk merubah tahun publikasi dari tipe data objek menjadi int muncul error sebagai berikut :


> **ValueError: invalid literal for int() with base 10: 'DK Publishing Inc'**

artinya terdapat value pada 'Year-Of-Publication' ada yang bernilai 'DK Publishing Inc'. Sepertinya ini terdapat kesalahan input, sehingga nanti akan dihapus nilai berupa teks tersebut sebelum mengubahnya ke dalam tipe data integer. Berdasarkan penelusuran, terdapat 2 nilai teks yaitu 'DK Publishing Inc' dan 'Gallimard'.
"""

books_df[(books_df['Year-Of-Publication'] == 'DK Publishing Inc') | (books_df['Year-Of-Publication'] == 'Gallimard')]

"""Jika diperhatikan pada kolom year of publication 3 buat data yang terisi data 'DK Publishing Inc' dan 'Gallimard' oleh karena itu akan di hapus value pada 'Year-Of-Publication' yang bernilai teks tersebut."""

temp = (books_df['Year-Of-Publication'] == 'DK Publishing Inc') | (books_df['Year-Of-Publication'] == 'Gallimard')
books_df = books_df.drop(books_df[temp].index)
books_df[(books_df['Year-Of-Publication'] == 'DK Publishing Inc') | (books_df['Year-Of-Publication'] == 'Gallimard')]

"""setelah 3 data Year-Of-Publication' yang bernilai teks sudah di hapus berikutnya akan Mengubah tipe data pada 'Year-Of-Publication' menjadi data type int."""

books_df['Year-Of-Publication'] = books_df['Year-Of-Publication'].astype(int)
print(books_df.dtypes)

"""Sekarang, tipe data pada 'Year-Of-Publication' sudah bertipe integer. Selanjutnya, adalah menghapus variabel yang tidak diperlukan pada proses pengembangan model. Karena nantinya pada sistem rekomendasi berbasis konten (content-based filtering) akan dibuat rekomendasi berdasarkan judul buku yang sama dengan nama penulis buku yang pernah dibaca oleh pengguna. Maka informasi seperti ukuran gambar tidak diperlukan, sehingga fitur/kolom 'Image-URL-S', 'Image-URL-M', dan 'Image-URL-L' bisa dihapus."""

# Menghapus kolom Image-URL semua ukuran
books_df.drop(labels=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1, inplace=True)

books_df.head()

"""Setelah di hapus kolom Image-URL sekarang dataset hanya tersisa 5 kolom/variabel saja. Untuk melihat berapa banyak entri dari masing - masing feature bisa di cek dibawah ini :"""

print("Jumlah nomor ISBN Buku:", len(books_df['ISBN'].unique()))
print("Jumlah judul buku:", len(books_df['Book-Title'].unique()))
print('Jumlah penulis buku:', len(books_df['Book-Author'].unique()))
print('Jumlah Tahun Publikasi:', len(books_df['Year-Of-Publication'].unique()))
print('Jumlah nama penerbit:', len(books_df['Publisher'].unique()))

"""Berdasarkan output diatas diketahui jumlah dari masing - masing feature. Perhatikan bahwa jumlah judul buku pada dataset yaitu 242.135 sedangkan jumlah nomor ISBN buku adalah 271.357, artinya ada beberapa buku yang tidak memiliki nomor ISBN, karena satu ISBN hanya boleh dimiliki oleh satu buku saja. Untuk Kasus ini nantinya dataset akan di filter agar setiap buku dipastikan memiliki satu nomor ISBN.

Selanjutnya, Dilakukan pengecekan distribusi data untuk melihat 10 nama penulis teratas berdasarkan jumlah buku.
"""

# Grouping'Book-Author' dan hitung jumlah buku yang ditulis oleh masing-masing penulis
author_counts = books_df.groupby('Book-Author')['Book-Title'].count()

# Urutkan penulis dalam urutan menurun
sorted_authors = author_counts.sort_values(ascending=False)

# Pilih 10 penulis teratas
top_10_authors = sorted_authors.head(10)

# Plot 10 penulis teratas dan buku yang ditulis oleh penulis kemudian dihitung menggunakan plot batang
plt.figure(figsize=(12, 6))
top_10_authors.plot(kind='bar')
plt.xlabel('Nama Penulis')
plt.ylabel('Jumlah Buku')
plt.title('10 Penulis Teratas Berdasarkan Jumlah Buku')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Berdasarkan informasi diatas, diketahui bahwa penulis dengan nama **Agatha Christie** menulis paling banyak buku yaitu sebanyak lebih dari **600 buku**. Dari informasi ini juga diketahui jika di dalam dataset terdapat beberapa nama penulis yang menulis buku lebih dari satu judul buku.

### **2. Dataset Ratings**

Selanjutnya, dilakukan eksplorasi pada Dataset ratings, yaitu penilaian terhadap buku dari pembaca atau pengguna. Dataset ratings ini nantinya akan digunakan untuk proses pengembangan model dengan collaborative filtering. Gunakan fungsi info() untuk melihat informasi dari variabel tersebut.
"""

ratings_df.info()

"""Berdasarkan output diatas, diketahui terdapat sebanyak 1.149.780 entri dan 3 kolom yaitu User-ID yang merupakan kode unik pengguna anonim yang memberikan peringkat, ISBN yang merupakan identitas berupa nomor unik buku, dan Book-Rating yang merupakan rating buku yang diberikan oleh pembaca atau pengguna. Untuk melihat jumlah entri dari masing - masing Feature, jalankan kode berikut."""

print('Jumlah User-ID:', len(ratings_df['User-ID'].unique()))
print('Jumlah buku berdasarkan ISBN:', len(ratings_df['ISBN'].unique()))

print('Jumlah rating buku:')
sorted_ratings = ratings_df['Book-Rating'].value_counts().sort_index()
pd.DataFrame({'Book-Rating': sorted_ratings.index, 'Jumlah': sorted_ratings.values})

"""Berdasarkan output diatas, diketahui jika terdapat 105.283 pengguna yang memberikan rating buku, jumlah buku berdasarkan ISBN yang diberikan rating adalah 340.556 buku, dan rating yang diberikan oleh masing - masing buku memiliki niliai berkisar antara 0 sampai 10, dimana 0 adalah rating paling rendah sedangkan 10 adalah rating paling tertinggi.

Seperti terlihat pada informasi sebelumnya, dataset ratings memiliki 1.149.780 baris data, dan itu merupakan jumlah yang sangat banyak. Nantinya, dataset rating ini yang akan digunakan dalam proses pengembangan model dengan collaborative filtering. Oleh karena itu, untuk menghemat alokasi memori pada saat pelatihan model nantinya, dataset rating ini tidak akan digunakan semua. Dataset rating hanya mengambil data pertama hingga data ke 5000 saja (exclude data ke 5000). Dataset ini akan digunakan untuk pengembangan model dengan collaborative filtering karena membutuhkan data rating terhadap pengguna untuk memberikan rekomendasi judul buku kepada pengguna lainnya. Untuk memudahkan supaya tidak tertukar dengan fitur lain yang serupa, datasetnya diubah namanya menjadi **df5000_rating**.
"""

df5000_rating = ratings_df[:20000]
df5000_rating

"""### **3. Dataset Users**

Dataset terakhir yang akan dilakukan eksplorasi adalah Dataset users. Dataset ini berisi informasi tentang pengguna anonim beserta demografinya. Gunakan fungsi info() untuk melihat informasi variabel.
"""

users_df.head()

users_df.info()

"""Berdasarkan informasi diatas, diketahui terdapat 278.858 entri dan terdapat 3 feature yaitu User-ID yang merupakan kode unik dari pengguna anonim, Location yang merupakan lokasi pengguna, dan Age yang merupakan usia pengguna. Diketahui juga terdapat beberapa pengguna yang usianya tidak diketahui. Data user berguna jika ingin membuat sistem rekomendasi berdasarkan demografi atau kondisi sosial pengguna. Namun, untuk studi kasus kali ini, tidak akan digunakan data users pada model. Pada pengembangan model, data yang digunakan adalah data books dan ratings.

### **Data Preprocessing**

Seperti yang sudah diketahui berdasarkan tahapan data understanding bahwa folder **books_data** Dataset terdiri dari 3 file terpisah yaitu books, ratings, dan users. Pada tahap ini, akan dilakukan proses penggabungan file menjadi satu kesatuan file agar sesuai dengan pengembangan model yang ingin dibuat.

### **Menggabungkan file dan Mengetahui Jumlah Rating**

Pada tahap ini dilakukan penggabungan file books dan ratings untuk mengetahui jumlah seluruh rating dari berbagai file tersebut. Implementasikan kode berikut.
"""

# Menggabungkan dataframe ratings dengan books berdasarkan nilai ISBN
books_df = pd.merge(ratings_df, books_df, on='ISBN', how='left')
books_df

"""Dataset setelah dilakukan penggabungan menjadi 7 feature dengan 1.149.780 baris data. Output diatas hanya menampilkan beberapa baris awal dan baris akhir data. Dataset inilah yang akan digunakan untuk membuat sistem rekomendasi. Selanjutnya, dilakukan perhitungan jumlah rating berdasarkan ISBN melalui kode berikut."""

books_df[['ISBN', 'User-ID', 'Book-Rating']].groupby('ISBN').agg({'User-ID': 'unique', 'Book-Rating': 'sum'})

"""# **Data Preparation**

### **Data Preparation Untuk Model Pengembangan dengan Content Based Filtering**

Pada tahap ini di akan dilakukan beberapa teknik untuk mempersiapkan data seperti:

1. Menghilangkan missing value.
2. Menyamakan jenis buku berdasarkan ISBN.

Pada sistem rekomendasi berbasis konten (content-based filtering) yang akan dikembangkan, satu nomor ISBN mewakili satu judul buku, yang artinya nomor ISBN pada setiap buku bersifat unik. Sehingga perlu dipersiapkan terlebih dahulu datanya agar siap untuk digunakan pada proses pelatihan model.

### **1. Mengatasi Missing Value**

Setelah proses penggabungan file, langkah selanjutnya adalah dilakukan proses pengecekan apakah ada missing value atau tidak. Jalankan kode berikut.
"""

# Cek missing value dengan fungsi isnull()
books_df.isnull().sum()

"""Terdapat banyak missing value pada sebagian besar fitur. Hanya fitur User-ID, ISBN, dan Book-Rating saja yang memiliki 0 missing value. Jumlah mising value terbesar ada di fitur 'Publisher' dan 'Book-Author' yaitu sebesar 118.650. 118.650 dari total dataset yaitu 1.149.780 merupakan jumlah yang tidak terlalu signifikan atau masih tergolong kecil. Oleh karena itu, untuk kasus ini akan dilakukan proses drop atau penghapusan pada missing value ini dan buatkan dalam bentuk Dataset baru bernama **books_clean**."""

books_clean = books_df.dropna()
books_clean

"""Sekarang, dataset terdiri dari 1.031.128 baris. Untuk memastikan tidak ada missing value lagi dalam data, jalankan kode berikut."""

books_clean.isnull().sum()

"""Sekarang, dataset sudah bersih dan bisa lanjut ke tahap berikutnya.

### **Menyamakan jenis buku berdasarkan ISBN**

Sebelum masuk tahap pemodelan, diperlukan proses menyamakan judul buku berdasarkan ISBN-nya. Jika terdapat nomor ISBN yang sama pada lebih dari satu judul buku dapat menyebabkan bias pada data. Oleh karena itu harus dipastikan bahwa hanya terdapat satu nomor ISBN pada satu judul buku saja.

mengecek berapa jumlah nomor ISBN Unik
"""

len(books_clean['ISBN'].unique())

"""mengecek berapa jumlah nomor Judul Buku yang unik"""

len(books_clean['Book-Title'].unique())

"""Berdasarkan informasi diatas, diketahui bahwa jumlah nomor ISBN dengan jumlah judul buku tidak sama, artinya terdapat nomor ISBN yang sama pada lebih dari satu judul buku. Hal tersebut harus diatasi dengan mengubah datasetnya menjadi data unik sehingga nantinya siap dimasukkan ke dalam proses pemodelan. Oleh karena itu, diperlukan proses membuang data duplikat pada kolom 'ISBN' dan simpan ke dalam variabel baru bernama **'prep_book'**. implementasikan kode berikut."""

prep_book = books_clean.drop_duplicates('ISBN')
prep_book

"""Setelah itu, kita lakukan proses pengecekan kembali jumlah data dari ISBN, judul buku (Book-Title), dan nama penulis buku (Book-Author). Dilakukan proses konversi data series menjadi list dengan fungsi tolist() dari library. Implementasikan kode berikut."""

# konversi data series 'ISBN' menjadi bentuk list
isbn_id = prep_book['ISBN'].tolist()

# konversi data series 'Book-Title' menjadi bentuk list
book_title = prep_book['Book-Title'].tolist()

# konversi data series 'Book-Author' menjadi bentuk list
book_author = prep_book['Book-Author'].tolist()

# konversi data series 'Year-Of-Publication' menjadi bentuk list
year_of_publication = prep_book['Year-Of-Publication'].tolist()

# konversi data series 'Publisher' menjadi bentuk list
publisher = prep_book['Publisher'].tolist()

print(len(isbn_id))
print(len(book_title))
print(len(book_author))
print(len(year_of_publication))
print(len(publisher))

"""Berdasarkan output diatas, diketahui bahwa sekarang jumlah data dari ISBN, judul buku, nama penulis buku, tahun publikasi dan nama penerbit sudah sama atau sudah merupakan data unik dan dataset hanya tersisa 270.144 baris data setelah melewati proses penghilangan duplikasi nilai. Tahap berikutnya yaitu pembuatan dictionary untuk menentukan pasangan key-value pada data isbn_id, book_title, book_author, year_of_publication, dan publihser yang sudah disiapkan sebelumnya untuk proses pengembangan model sistem rekomendasi berbasis konten (content-based filtering)."""

# Membuat dictionary untuk data ‘isbn_id’, ‘book_title’, ‘book_author’, 'year_of_publication', dan 'publisher'
books_ok = pd.DataFrame({
    'isbn': isbn_id,
    'book_title': book_title,
    'book_author': book_author,
    'year_of_publication': year_of_publication,
    'publisher': publisher

})

books_ok

"""Karena dataset yang dimiliki terlalu banyak dan secara otomatis alokasi memori yang digunakan nantinya akan sangat banyak untuk memproses seluruh data pada pengembangan model, maka pada proyek ini hanya akan mengambil data pertama hingga data ke 20.000 (exlude data ke 20.000)."""

books_ok = books_ok[:20000]
books_ok

"""Inilah data yang akan digunakan pada proses pengembangan model dengan teknik content based filtering.

### **Data Preparation Untuk Model Pengembangan dengan Collaborative Filtering**

Pada model pengembangan dengan collaborative filtering, data nantinya akan dibagi menjadi data training dan data validasi dalam proses pelatihan model, sebelum di bagi menjadi data training dan data validasi, data harus dipersiapkan terlebih dahulu. Data rating harus diubah ke dalam bentuk matriks numerik agar nantinya mempermudah proses pelatihan model sehingga model menjadi mudah mengenali/mempelajari data tersebut. Sebelum itu dilakukan, Pada tahap ini di dilakukan beberapa teknik untuk mempersiapkan data seperti menyandikan (encode) fitur 'User-ID' dan 'ISBN' ke dalam indeks integer, memetakan 'User-ID' dan 'ISBN'ke dataframe yang berkaitan, dan yang terakhir mengecek beberapa hal dalam data seperti jumlah pengguna, jumlah buku, kemudian mengubah nilai rating menjadi float agar bisa digunakan pada proses pelatihan model.

Pertama, dilakukan proses menyandikan (encode) fitur 'User-ID' dan 'ISBN' ke dalam indeks integer. Sebagai Berikut :
"""

# mengubah User-ID menjadi list tanpa nilai yang sama
user_ids = df5000_rating ['User-ID'].unique().tolist()
print('list userID: ', user_ids)

# melakukan encoding User-ID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID: ', user_to_user_encoded)

# melakukan proses encoding angka ke User-ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""Selanjutnya, lakukan hal yang sama pada 'ISBN'."""

# mengubah ISBN menjadi list tanpa nilai yang sama
isbn_id = df5000_rating['ISBN'].unique().tolist()
print('list ISBN_Id: ', isbn_id)

# melakukan encoding ISBN
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_id)}
print('encoded ISBN_Id: ', isbn_to_isbn_encoded)

# melakukan proses encoding angka ke ISBN
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_id)}
print('encoded angka ke ISBN_Id: ', isbn_encoded_to_isbn)

"""Berikutnya, petakan User-ID dan ISBN ke dataframe yang berkaitan.

Menonaktifkan peringatan SettingWithCopyWarning
"""

pd.options.mode.chained_assignment = None

"""Mapping User-ID ke dataframe user"""

df5000_rating['user'] = df5000_rating['User-ID'].map(user_to_user_encoded)

"""Mapping ISBN ke dataframe judul buku"""

df5000_rating['book_title'] = df5000_rating['ISBN'].map(isbn_to_isbn_encoded)

"""Cek beberapa hal dalam data seperti jumlah user, jumlah judul buku, dan mengubah nilai rating menjadi float."""

# mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# mendapatkan jumlah judul buku
num_book_title = len(isbn_to_isbn_encoded)
print(num_book_title)

# mengubah rating menjadi nilai float
df5000_rating['Book-Rating'] = df5000_rating['Book-Rating'].values.astype(np.float32)

# nilai minimum rating
min_rating = min(df5000_rating['Book-Rating'])

# nilai maksimum rating
max_rating = max(df5000_rating['Book-Rating'])

print('Number of User: {}, Number of Book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book_title, min_rating, max_rating
))

"""Tahap persiapan data sudah selesai. Data sudah siap untuk digunakan nantinya pada proses pembagian menjadi data training dan data validasi pada proses pengembangan model dengan collaborative filtering.

# **Modeling**

## **Model Development dengan Content Based Filtering**

Pada tahap ini, akan dikembangkan model dengan teknik Content Based Filtering. Content Based Filtering adalah salah satu pendekatan dalam sistem rekomendasi yang menggunakan informasi atau "konten" dari item atau pengguna untuk membuat rekomendasi. Ide dasarnya adalah mencocokkan preferensi pengguna dengan karakteristik atau konten dari item yang telah dilihat atau disukai oleh pengguna sebelumnya. Misalkan, jika seorang pengguna menyukai atau pernah membeli buku dengan judul "Introduction to Machine Learning" dan buku tersebut memiliki fitur berupa nama penulis buku yaitu "Alex Smola", maka sistem akan mencari buku lain dengan fitur serupa dan merekomendasikannya dalam bentuk top-N recommendation kepada pengguna tersebut.

Pada proses pengembangan model dilakukan pencarian representasi fitur penting dari setiap judul buku dengan TF-IDF (Term Frequency - Inverse Document Frequency) Vectorizer. TF-IDF vectorizer adalah alat yang digunakan untuk mengonversi dokumen teks menjadi representasi vektor berdasarkan nilai TF-ID setiap kata dalam dokumen tersebut. TF (Term Frequency) mengukur seberapa sering suatu kata muncul dalam suatu dokumen. Sedangkan, IDF mengukur seberapa unik atau jarang suatu kata muncul dalam seluruh koleksi dokumen. Vektor ini nanti digunakan untuk melakukan proses pencarian representasi fitur penting dari setiap judul buku berdasarkan nama penulis buku pada model yang dikembangkan dengan teknik Content Based Filtering. Pada proyek ini digunakan fungsi tfidfvectorizer() dari library Sklearn.

Sementara itu, untuk menghitung derajat kesamaan (similarity degree) antar judul buku digunakan teknik cosine similarity. Metode ini digunakan untuk mengukur sejauh mana kesamaan antar dua vektor dalam ruang berdimensi banyak. Cosine similarity mengukur sudut kosinus antara dua vektor, dan semakin kecil sudutnya, semakin besar kesamaan antara vektor - vektor tersebut. Pada proyek ini digunakan fungsi cosine_similarity dari library Sklearn.

### **TF-IDF Vectorizer**

Import fungsi tfidfvectorizer() dari libray Sklearn.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data book_author
tf.fit(books_ok['book_author'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

"""Selanjutnya, lakukan fit dan transformasi ke dalam bentuk matriks."""

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(books_ok['book_author'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Berdasarkan output, matrik memiliki ukuran (20000, 8829). Nilai 20000 merupakan ukuran data dan 8829 merupakan matriks nama penulis buku. Untuk menghasilkan vektor tf-idf dalam bentuk matriks, gunakan fungsi todense()."""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""Selanjutnya, mari lihat matriks tf-idf untuk beberapa judul buku dan nama penulis buku dalam bentuk dataframe, dimana kolom diisi dengan nama penulis buku sedangkan baris diisi dengan judul buku."""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=books_ok.book_title
).sample(15, axis=1).sample(10, axis=0)

"""Output dari matriks tf-idf berhasil mengidentifikasi representasi fitur penting dari setiap kategori judul buku dengan fungsi tfidfvectorizer. Pada Kasus ini dataset hanya ditampilkan berupa sampel data sehingga tidak terlihat keseluruhan matriks. Dari 20000 data hanya dipilih sampel data acak yang terdiri dari 10 judul buku pada baris vertikal dan 15 nama penulis buku pada baris horizontal.

### **Cosine Similarity**

Berikutnya mengidentifikasi korelasi antara judul buku dengan penulis buku. Sekarang, akan dilakukan proses menghitung derajat kesamaan (similarity degree) antar judul buku dengan teknik cosine similarity.
"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine = cosine_similarity(tfidf_matrix)
cosine

"""Pada tahap ini, dilakukan proses perhitungan cosine similarity dataframe tfidf_matrix yang diperoleh pada tahapan sebelumnya. Dengan fungsi cosine_similarity dari library sklearn, didapat nilai kesamaan (similarity) antar judul buku. Kode diatas menghasilkan keluaran berupa matriks kesamaan dalam bentuk array.

Selanjutnya, mari lihat matriks kesamaan setiap judul buku dengan menampilkan nama judul buku dalam 5 sampel kolom (axis = 1) dan 10 sampel baris (axis = 0).
"""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama judul buku
cosine_df = pd.DataFrame(cosine, index=books_ok['book_title'], columns=books_ok['book_title'])
print('Shape:', cosine_df.shape)

# Melihat similarity matrix pada setiap judul buku
cosine_df.sample(5, axis=1).sample(10, axis=0)

"""Dengan cosine similarity, berhasil mengidentifikasi kesamaan antara satu judul buku dengan judul buku lainnya. Shape (20000, 20000) merupakan ukuran matriks similarity dari data. Berdasarkan data yang ada, matriks diatas sebenarnya berukuran 20000 judul buku x 20000 judul buku (masing - masing dalam sumbu X dan Y). Artinya, telah berhasil mengidentifikasi tingkat kesamaan pada 20000 judul buku. Tapi disini tidak bisa menampilkan semua datanya. Oleh karena itu, hanya dipilih 10 judul buku pada baris vertikal dan 5 judul buku pada baris horizontal. Dengan data kesamaan (similarity) judul buku yang diperoleh sebelumnya, akan dilakukan proses rekomendasi daftar judul buku yang mirip dengan judul buku yang sebelumnya pernah dibeli atau dibaca oleh pengguna.

### **Memperoleh Informasi Rekomendasi**

Pada tahap ini, akan dibuatkan sebuah fungsi bernama Rekomendasi_buku dengan beberapa parameter sebagai berikut:

1. book_title : nama judul buku (index kemiripan dataframe).
2. similarity_data : Dataframe mengenai similarity yang telah didefinisikan sebelumnya.
3. items : Nama dan fitur yang digunakan untuk mendefinisikan kemiripan, dalam hal ini adalah 'book_title' dan 'book_author'.
4. k : Jumlah top-N recommendation yang diberikan oleh sistem rekomendasi. Secara default, k bernilai 5.

Sebelum menulis kode, perlu diingat bahwa definisi dari sistem rekomendasi yang menyatakan bahwa keluaran sistem adalah berupa top-N recommendation. Oleh karena itu, perlu diberikan sejumlah rekomendasi judul buku pada pengguna yang diatur pada parameter k.
"""

def rekomendasi_buku(book_title, similarity_data=cosine_df, items=books_ok[['book_title', 'book_author']], k=5):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,book_title].to_numpy().argpartition(range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop book_title agar nama buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(book_title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""proses pengambilan sejumlah nilai k tertinggi dari similarity data (dalam kasus ini: dataframe **cosine_df**). Selanjutnya, mengambil data dari bobot (tingkat kesamaan) tertinggi ke terendah. Data ini dimasukkan ke dalam variabel closest. Berikutnya, perlu dihapus book_title yang dicari agar tidak muncul dalam daftar rekomendasi. Dalam kasus ini, akan dicari judul buku yang mirip dengan judul buku yang nanti di input dalam book_title, sehingga perlu drop book_title agar tidak muncul dalam daftar rekomendasi yang diberikan nanti.

Gunakan fungsi **rekomendasi_buku** tersebut untuk membuat rekomendasi 10 buku teratas yang direkomendasikan oleh sistem.
"""

book_title_test = "A Caribbean Mystery" # contoh judul buku

books_ok[books_ok.book_title.eq(book_title_test)]

"""Perhatikan, bahwa judul buku **'A Caribbean Mystery'** ditulis oleh **'Agatha Christie'**. Sekarang, gunakan fungsi buku_rekomendasi untuk mendapatkan rekomendasi berdasarkan judul buku tersebut."""

# Mendapatkan rekomendasi judul buku yang mirip
rekomendasi_buku(book_title_test)

"""Berdasarkan output diatas, sistem berhasil merekomendasikan 5 judul buku teratas dengan kategori nama penulis (book_author) yaitu **'Agatha Christie'**.

### **Model Development dengan Collaborative Filtering**

Pada proses pengembangan model kali ini, akan diterapkan teknik collaborative filtering untuk membuat sistem rekomendasi. Teknik ini membutuhkan data rating dari pengguna atau pembaca. Collaborative filtering adalah salah satu metode dalam sistem rekomendasi yang memprediksi preferensi atau minat pengguna terhadap item berdasarkan informasi dari pengguna lain (kolaborasi). Ide dasar dibalik collaborative filtering adalah bahwa pengguna yang memiliki preferensi serupa dalam masa lalu cenderung memiliki preferensi serupa untuk item di masa depan. Pada kasus ini akan dibuat model collaborative filtering berdasarkan kesamaan antar pengguna (User-Based Collaborative Filtering).

Pengembangan model dengan Collaborative filtering pada kasus ini akan menghasilkan rekomendasi sejumlah judul buku yang sesuai dengan preferensi pengguna berdasarkan rating yang telah diberikan sebelumnya. Dari data rating pengguna, akan diidentifikasi nama - nama judul buku yang mirip dan belum pernah dibaca atau dibeli oleh pengguna untuk direkomendasikan.

Setelah tahapan persiapan data untuk pengembangan model ini sudah dilakukan di bagian data preparation sebelumnya, langkah selanjutnya dilakukan pembagian data untuk data training dan data validasi, kemudian dilanjutkan dengan proses training model. Pada proses training, model menghitung skor kecocokan antara pengguna dan judul buku dengan teknik embedding. Pertama, dilakukan proses embedding terhadap data pengguna dan judul buku. Selanjutnya, lakukan operasi perkalian dot product antara embedding pengguna dan judul buku. Selain itu, ditambahkan bias untuk setiap pengguna dan judul buku. Skor kecocokan ditetapkan dalam skala [0,1] dengan fungsi aktivasi sigmoid. Model dibuatkan class RecommenderNet dengan keras Model class. Kode class RecommenderNet ini terinspirasi dari tutorial dalam situs keras dengan beberapa adaptasi layer yang menyesuaikan dengan kasus yang sedang dikerjakan. Model akan menggunakan Binary Crossentropy untuk menghitung loss function, Adam (Adaptive Moment Estimation) sebagai optimizer, dan Root Mean Squared Error (RMSE) sebagai metrik evaluasi.

### **Membagi data untuk Training dan Validasi**

Sebelum dilakukan pembagian data menjadi training dan validasi, data terlebih dahulu diacak agar distribusinya menjadi random.
"""

# mengacak dataset
df5000_rating = df5000_rating.sample(frac=1, random_state=42)
df5000_rating

"""Selanjutnya, dilakukan proses pembagian data menjadi data train dan validasi dengan komposisi 80:20. Namun sebelumnya, perlu dipetakan (mapping) data user dan judul buku menjadi satu value terlebih dahulu. Kemudian, dibuat rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training."""

# membuat variabel x untuk mencocokkan data user dan judul buku menjadi satu value
x = df5000_rating[['user', 'book_title']].values

# membuat variabel y untuk membuat rating dari hasil
y = df5000_rating['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# membagi menjadi 90% data train dan 10% data validasi

train_indices = int(0.8 * df5000_rating.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""Sampai proses ini, data sudah siap untuk dimasukkan ke dalam pengembangan model dengan collaborative filtering.

### **Proses training**

Pada proses training model, model akan menghitung skor kecocokan antara pengguna dan judul buku dengan teknik embedding. Pertama, dilakukan proses embedding terhadap data user dan book_title. Selanjutnya, lakukan operasi perkalian dot product antara embedding user dan book_title. Selain itu, juga dapat menambahkan bias untuk setiap user dan book_title. Skor kecocokan ditetapkan dalam skala [0, 1] dengan fungsi aktivasi sigmoid.

Di sini, Model dibuatkan class RecommenderNet dengan keras Model class. Kode class RecommenderNet ini terinspirasi dari tutorial dalam situs keras dengan beberapa adaptasi layer yang menyesuaikan dengan kasus yang sedang dikerjakan.
"""

class RecommenderNet(tf.keras.Model):

    # inisialisasi fungsi
    def __init__(self, num_users, num_book_title, embedding_size, dropout_rate=0.2, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_book_title = num_book_title
        self.embedding_size = embedding_size
        self.dropout_rate = dropout_rate

        self.user_embedding = layers.Embedding( # layer embedding user
            num_users,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias

        self.book_title_embedding = layers.Embedding( # layer embedding book_title
            num_book_title,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6)
        )
        self.book_title_bias = layers.Embedding(num_book_title, 1) # layer embedding book_title

        self.dropout = layers.Dropout(rate=dropout_rate)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0]) # memanggil layer embedding 1
        user_vector = self.dropout(user_vector)
        user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2

        book_title_vector = self.book_title_embedding(inputs[:, 1]) # memanggil layer embedding 3
        book_title_vector = self.dropout(book_title_vector)
        book_title_bias = self.book_title_bias(inputs[:, 1]) # memanggil layer embedding 4

        dot_user_book_title = tf.tensordot(user_vector, book_title_vector, 2) # perkalian dot product

        x = dot_user_book_title + user_bias + book_title_bias

        return tf.nn.sigmoid(x) # activation sigmoid

"""Selanjutnya, lakukan proses compile terhadap model."""

model = RecommenderNet(num_users, num_book_title, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=1e-4),
    metrics = [tf.keras.metrics.RootMeanSquaredError()]
)

"""Model ini menggunakan Binary Crossentropy untuk menghitung loss function, Adam (Adaptive Moment Estimation) sebagai optimizer, dan root mean squared error (RMSE) sebagai metrics evaluation."""

# memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 16,
    epochs = 50,
    validation_data = (x_val, y_val)
)

"""Berdasarkan hasil proses training model, didapat hasil yang cukup memuaskan dan model konvergen pada epochs sekitar 50. Dari proses ini, diperoleh nilai Root Mean Squared Error (RMSE) sebesar sekitar 0.2953 dan RMSE pada data validasi sebesar 0.3385. Nilai ini cukup bagus untuk sistem rekomendasi. Untuk mengetahui hasil dari pengembangan model, langkah selanjutnya adalah mendapatkan rekomendasi judul buku berdasarkan model yang dikembangan.

### **Mendapatkan Rekomendasi Judul Buku**

Untuk mendapatkan rekomendasi judul buku, pertama diambil sampel user secara acak dan definisikan variabel **book_not_read** yang merupakan daftar buku yang belum pernah dibaca atau dibeli oleh pengguna. variabel **book_not_read** inilah yang akan menjadi judul buku yang direkomendasikan oleh sistem.

Variabel **book_bot_visit** diperoleh dengan menggunakan operator bitwise (~) pada variabel **book_read_by_user**.
"""

book_colab = books_ok

# mengambil sampel user
user_id = df5000_rating['User-ID'].sample(1).iloc[0]
book_read_by_user = df5000_rating[df5000_rating['User-ID'] == user_id]

# membuat variabel book_not_readed
book_not_read = book_colab[~book_colab['isbn'].isin(book_read_by_user['ISBN'].values)]['isbn']
book_not_read = list(
    set(book_not_read)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)

book_not_read = [[isbn_to_isbn_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

"""Selanjutnya, untuk memperoleh rekomendasi judul buku, gunakan fungsi model.predict() dari library Keras."""

ratings_model = model.predict(user_book_array).flatten()

top_ratings_indices = ratings_model.argsort()[-10:][::-1]

recommended_book_ids = [
    isbn_encoded_to_isbn.get(book_not_read[x][0]) for x in top_ratings_indices
]

top_book_user = (
    book_read_by_user.sort_values(
        by='Book-Rating',
        ascending=False
    )
    .head(10)['ISBN'].values
)

book_df_rows = book_colab[book_colab['isbn'].isin(top_book_user)]

# Menampilkan rekomendasi buku dalam bentuk DataFrame
book_colab_rows_data = []
for row in book_df_rows.itertuples():
    book_colab_rows_data.append([row.book_title, row.book_author])

recommended_book = book_colab[book_colab['isbn'].isin(recommended_book_ids)]

recommended_book_data = []
for row in recommended_book.itertuples():
    recommended_book_data.append([row.book_title, row.book_author])

# Membuat DataFrame untuk output
output_columns = ['Book Title', 'Book Author']
df_book_read_by_user = pd.DataFrame(book_colab_rows_data, columns=output_columns)
df_recommended_books = pd.DataFrame(recommended_book_data, columns=output_columns)

# Menampilkan hasil rekomendasi dalam bentuk DataFrame
print("Showing recommendation for users: {}".format(user_id))
print("===" * 9)
print("Book with high ratings from user")
print("----" * 8)
print(df_book_read_by_user)
print("----" * 8)
print("Top 10 books recommendation")
print("----" * 8)
df_recommended_books

"""Berdasarkan output diatas, telah berhasil dibuat rekomendasi kepada user. Hasil di atas adalah rekomendasi untuk user dengan id 32. Dari output tersebut, dapat dibandingkan antara 'Book with high ratings from user' dan 'Top 10 books recomendation' untuk user.

Beberapa judul buku rekomendasi menyediakan nama penulis bukunya juga yang sesuai dengan rating user. Diperoleh 10 top rekomendasi buku yang disertai juga dengan nama penulisnya untuk user tersebut serta terdapat 1 judul buku yang merupakan buku dengan rating tertinggi dari user.

## **Evaluation**

### **Evaluasi Model dengan Content Based Filtering**

Metrik yang digunakan untuk evaluasi model dengan content based filtering di kasus kali ini adalah Precision, Recall, dan F1-Score. Metrik ini adalah metrik yang umum digunakan untuk mengukur kinerja model. Precision merupakan rasio item yang revelan yang dihasilkan oleh model terhadap total item yang dihasilkan. Recall merupakan rasio item relevan yang dihasilkan oleh model terhadap total item yang seharusnya direkomendasikan. Sedangkan, F1 Score adalah gabungan dari Precision dan Recall, memberikan nilai tunggal yang mengukur keseimbangan antara keduanya.

Sebelum menghitung nilai evaluasi metrik menggunakan precision, recall dan f1 score, diperlukan sebuah data yang terdiri dari label sebenarnya dan digunakan untuk menilai hasil prediksi model, data ini disebut sebagai data ground truth. Data ground truth pada proyek ini dibuat menggunakan hasil derajat kesamaan yang dihitung menggunakan teknik cosine similarity, dimana setiap baris dan kolom mewakili judul buku, dan nilai di setiap sel pada dataframe mewakili label. Angka 1 untuk similar, dan angka 0 untuk tidak similar. Perlu ditetapkan juga sebuah nilai ambang batas atau threshold untuk memutuskan apakah nilai similarity antara dua item harus dianggap 1 (similar) atau 0 (tidak similar).
"""

# Menentukan threshold untuk mengkategorikan similarity sebagai 1 atau 0
threshold = 0.5

# Membuat ground truth data dengan asumsi threshold
ground_truth = np.where(cosine >= threshold, 1, 0)

# Menampilkan beberapa nilai pada ground truth matrix
ground_truth_df = pd.DataFrame(ground_truth, index=books_ok['book_title'], columns=books_ok['book_title']).sample(5, axis=1).sample(10, axis=0)

"""Pada kode diatas, ditetapkan nilai ambang batas atau threshold sebesar 0.5. Nilai threshold ini disesuaikan dengan kebutuhan dan karakteristik setelah melihat hasil rekomendasi sebelumnya. Lalu dibuatkan matriks ground truth menggunakan fungsi np.where() dari NumPy. Matriks ini akan memiliki nilai 1 di posisi di mana nilai cosine similarity antara dua item lebih besar atau sama dengan nilai threshold yang ditetapkan, dan nilai 0 di posisi di mana nilai similarity di bawah threshold. Kemudian, setelah matriks dibuat, hasilnya disajikan dalam bentuk dataframe. Baris dan kolom Dataframe ground truth ini diindeks menggunakan judul buku dari data. Berikut tampilan dari dataframe ground truth."""

ground_truth_df

"""Setelah dibuatkan matriks ground truth yang berisi label sebenarnya dari hasil cosine similarity. Selanjutnya, dilakukan proses perhitungan evaluasi model dengan metrik precision, recall, dan f1 score. Pertama, mengimport fungsi precision_recall_fscore_support dari library Sklearn yang digunakan untuk menghitung precision, recall dan f1 score. Lalu karena keterbatasan alokasi memori pada perangkat, data hanya diambil sekitar 10000 sampel dari cosine similarity dan ground truth matriks. Hal ini dilakukan untuk mempercepat proses perhitungan, terutama karena ukuran matriks yang cukup besar. Kemudian, matriks cosine similarity dan ground truth dikonversi menjadi array satu dimensi agar mempermudah perbandingan dan perhitungan metrik evaluasi.

Digunakan juga threshold untuk mengkategorikan nilai cosine similarity sebagai 1 atau 0. Jika nilai similarity di atas atau sama dengan threshold, dianggap sebagai 1 (positif), dan jika di bawah threshold, dianggap sebagai 0 (negatif). Hasilnya disimpan dalam array predictions. Terakhir, digunakan fungsi precision_recall_fscore_support untuk menghitung precision, recall, dan f1 score. Parameter average='binary' digunakan karena sedang mengukur kinerja dalam konteks klasifikasi biner (1 atau 0). Parameter 'zero_division=1' digunakan untuk menghindari pembagian dengan nol jika ada kelas yang tidak terdapat di prediksi. Implementasi kode berikut:
"""

from sklearn.metrics import precision_recall_fscore_support

# Mengambil sebagian kecil dari cosine similarity matrix dan ground truth matrix
sample_size = 10000
cosine_sim_sample = cosine[:sample_size, :sample_size]
ground_truth_sample = ground_truth[:sample_size, :sample_size]

# Mengonversi cosine similarity matrix menjadi array satu dimensi untuk perbandingan
cosine_sim_flat = cosine_sim_sample.flatten()

# Mengonversi ground truth matrix menjadi array satu dimensi
ground_truth_flat = ground_truth_sample.flatten()

# Menghitung metrik evaluasi
predictions = (cosine_sim_flat >= threshold).astype(int)
precision, recall, f1, _ = precision_recall_fscore_support(
    ground_truth_flat, predictions, average='binary', zero_division=1
)

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""Berdasarkan hasil evaluasi, didapat nilai dari masing - masing metrik evaluasi yaitu precision, recall dan F1 Score. Nilai Precision didapat sebesar 1.0, artinya semua prediksi positif model adalah benar dan tidak terdapat false positive. Nilai recall didapat nilai 1.0 menunjukkan bahwa model berhasil mengidentifikasi sekitar 100% dari semua item yang sebenarnya relevan. Nilai F1 Score didapat sekitar 1.0 juga, ini menunjukkan keseimbangan yang baik antara precision dan recall dan model cenderung memberikan hasil yang sangat baik untuk kedua kelas (positif dan negatif). Kesimpulannya, berdasarkan hasil metrik evaluasi tersebut model bekerja dengan sangat baik dalam memberikan rekomendasi item dengan content based filtering.

### **Evaluasi Model dengan Collaborative Filtering**

Seperti yang sudah dilihat pada proses pelatihan model di bagian modeling. Metrik yang digunakan untuk melakukan evaluasi model pada model dengan Collaborative Filtering di proyek ini adalah Root Mean Squared Error (RMSE). RMSE adalah metrik evaluasi yang umum digunakan untuk mengukur seberapa baik model memprediksi nilai kontinu dengan membandingkan nilai prediksi dengan nilai sebenarnya. Dalam konteks collaborative filtering, RMSE biasanya digunakan untuk mengukur seberapa baik model kolaboratif dalam memprediksi preferensi pengguna terhadap item.

Berdasarkan hasil proses training model pada tahap modeling, diperoleh hasil pelatihan berupa informasi RMSE di data train dan validasi. Untuk melihat visualisai proses training model, dilakukan proses plot metrik evaluasi dengan matplotlib. Terapkan kode berikut.
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Berdasarkan hasil visualisasi metrik evaluasi RMSE terhadap model yang dikembangkan, terlihat hasil model konvergen pada epochs sekitar 50 dan berdasarkan plot metriks model terlihat memberikan nilai MSE yang cukup kecil. Dari proses ini, diperoleh nilai error akhir sebesar 0.2953  dan error pada data validasi sebesar 0.3385. Nilai tersebut menunjukkan hasil yang cukup baik untuk sistem rekomendasi yang dihasilkan. Semakin kecil nilai RMSE, semakin baik model dalam memprediksi preferensi pengguna terhadap item. Hal inilah yang menyebabkan hasil rekomendasi dari model cukup akurat."""